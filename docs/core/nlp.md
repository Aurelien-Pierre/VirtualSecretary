::: core.nlp

## Examples

```python
from nlp import *

# Build the training set of Data
nltk.download('nps_chat')
training_set = [Data(post.text, post.get('class')) for post in nltk.corpus.nps_chat.xml_posts()]
embedding_set = [post.text for post in nltk.corpus.nps_chat.xml_posts()]

# Build the model
model = Classifier(training_set, embedding_set, 'sentences_classifier.joblib', False)

# Test word2vec
print(model.word2vec.wv.most_similar("free"))

# Classify test stuff
l = ["Do you have time to meet at 5 pm ?", "Come with me !", "Nope", "Well, no.", "What do you think ?"]

for item in l:
  print(model.classify(item))

print(model.classify_many(l))
print(model.prob_classify_many(l))
```

## Important notes

The machine-learning here uses 2 layers :

- a first layer of word embedding using [word2vec](https://en.wikipedia.org/wiki/Word2vec), which is an auto-supervised neural network intended to reduce words to vectors representing both their syntactical properties and their typical association patterns. Thes vectors allow to compute the distance between words: a low vector distance between words translates the fact that these words are very often found next to each other in the training text samples.
- a second layer of supervised machine learning using a [support vector machine (SVM)](https://en.wikipedia.org/wiki/Support_vector_machine) or random forest, which is a classifier taking vectors in input (which can be a set of boolean or quantitative features) and a label. Namely, the set of features used here is the vectors generated by the previous word2vec layer.

There are some things to understand here, because AI and machine learning work great in specific use cases but are not magical.

### Word2Vec is optional

Let's use a simple example sentence : `I think I'm not wrong`.

It is possible to process emails with only the SVM layer, by using boolean features, that is :

- `contains("I") = True`,
- `contains("think") = True`,
- `contains("m") = True`,
- `contains("not") = True`,
- `contains("wrong") = True`.

It is also possible to use quantitative features, that is :

- `contains("I") = 2` because "I" is counted twice,
- `contains("think") = 1`,
- `contains("m") = 1`,
- `contains("not") = 1`,
- `contains("wrong") = 1`.

Each of the `contains(X)` features is then turned into a vector dimension, and the sentence is then converted into a vector, that is `[2, 1, 1, 1, 1]` for the quantitative approach. We redo the same for all the sentences of the training set, so for example `But I'm not` would get `contains("I") = True` and `contains("wrong") = False`.

Then, we identify the most significant dimensions to reduce the complexity of the problem and solve the SVM fitting. This gives 75-82 % accuracy on bi-lingual emails (French & English), using 700 features, and works best on short sentences. I haven't found a signicative difference between the boolean and the quantitive approach.

In terms of emails, we can also add features based on metadata, like `contains(attachment)`.

### But Word2Vec is nice

The way word2vec breaks words into vectors cannot be directly matched to human-readable features like the previous, but the vectors themselves have meaning through their vector norm, that allows to compute actual quantitative distance between words, based on their probability of being associated.

The NLP module here uses vectors of 300 dimensions. Those vectors are summed for each word in the sentence and normalized (so, they are actually averaged), and then fed to the SVM classifier for fitting in place of the previous `contains(X)` features. The quality of the SVM fitting will then become dependent on the quality of the underlying word2vec model.

Using a bi-lingual training corpus of 30.600 samples for the word2vec, and a corpus of 24.000 emails for the SVM classifier, this gives an accuracy of 88-92%, which is remarkable given that we uses only 300 dimensions (instead of 700 previously). However, it yields no significant accuracy increase on short sentences and/or small training sets (6000 samples or less). But the reduced dimensionality of the problem leads to better computational runtimes, so it's still a win.

### What to choose ?

The NLP module here mandatorily uses word2vec, and bypassing it would require to modify the API. As your training set decreases (several thousands of samples), I find it's best to reduce the dimensions of the word2vec model to 100-150. Once you reach 15.000 samples, you can increase it to 200-300 dimensions.

Ultimately, it is all down to the quality, quantity and variety of the training sample. While the SVM classifier needs to be trained on the actual data you need classify, the word2vec layer can be trained on any piece of natural language. See [protocols.html_server][] for a web scrapping helper that will allow to you gather training sets on websites. Using human writings that use the vocabulary of your particular business will help tremendously to build an AI that speaks your language, especially if you use multiple languages.

When building models with the validation option, 95% of the training set you feed the AI will be actually used for training, the remaining 5% will be used to test the prediction performance of the model. Both accuracies are displayed in console output. You should use that information to tune your AI parameters, keeping in mind that an high accuracy in the training set will usually lead to overfitting and poor performance on the prediction (testing set). Since the 95/5 % splitting is done randomly, you may retry it a couple of times to see how accuracy averages.

[See a visual comparison of classifiers performance](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py), but keep in mind that the examples shown are in 2D, while we work here in 150 to 300D, so things might be a lot different.
